{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcFZZwrcD_qh"
      },
      "source": [
        "## Setup y carga del trabajo completo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BBCvc6tD5ZM",
        "outputId": "b5f7f8d8-6fa8-4c7f-86b6-0502b5d56b13"
      },
      "outputs": [],
      "source": [
        "# Importaciones y configuraci√≥n\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "from collections import defaultdict, Counter\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse import csr_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuraci√≥n de visualizaci√≥n\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "\n",
        "print(\"üöÄ SESI√ìN 5: AGENTE DE RECOMENDACI√ìN INTELIGENTE COMPLETO\")\n",
        "print(\"=\" * 70)\n",
        "print(\"Objetivo: Integrar todo el trabajo en un agente inteligente funcional\")\n",
        "print(\"Evoluci√≥n: Modelo Tradicional ‚Üí Arquitectura Modular ‚Üí RL Adaptativo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUOTpyPOEFhn"
      },
      "source": [
        "## Recrear y integrar trabajo de sesiones previas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltnW4CEwEIij",
        "outputId": "2ad10493-2efb-49c5-b1b6-23dd177b99ef"
      },
      "outputs": [],
      "source": [
        "# Cargar datos y recrear todos los componentes previos\n",
        "print(\"Integrando trabajo de sesiones previas...\")\n",
        "\n",
        "# Cargar datasets\n",
        "# Artistas\n",
        "artists = pd.read_csv('artists.dat', sep='\\t', encoding='latin-1')\n",
        "\n",
        "# Interacciones usuario-artista (el n√∫cleo de nuestro modelo)\n",
        "user_artists = pd.read_csv('user_artists.dat', sep='\\t', encoding='latin-1')\n",
        "\n",
        "# Tags disponibles\n",
        "tags = pd.read_csv('tags.dat', sep='\\t', encoding='latin-1')\n",
        "\n",
        "# Tags asignados por usuarios a artistas\n",
        "user_tagged = pd.read_csv('user_taggedartists.dat', sep='\\t', encoding='latin-1')\n",
        "\n",
        "# Red social de amigos\n",
        "user_friends = pd.read_csv('user_friends.dat', sep='\\t', encoding='latin-1')\n",
        "\n",
        "user_tagged['date'] = pd.to_datetime(user_tagged[['year', 'month', 'day']])\n",
        "\n",
        "# SESI√ìN 1: Modelo tradicional (baseline)\n",
        "def create_traditional_baseline():\n",
        "    \"\"\"Recrear modelo SVD tradicional\"\"\"\n",
        "    user_ids = sorted(user_artists['userID'].unique())\n",
        "    artist_ids = sorted(user_artists['artistID'].unique())\n",
        "    user_to_idx = {uid: idx for idx, uid in enumerate(user_ids)}\n",
        "    artist_to_idx = {aid: idx for idx, aid in enumerate(artist_ids)}\n",
        "\n",
        "    rows, cols, data = [], [], []\n",
        "    for _, row in user_artists.iterrows():\n",
        "        user_idx = user_to_idx[row['userID']]\n",
        "        artist_idx = artist_to_idx[row['artistID']]\n",
        "        rows.append(user_idx)\n",
        "        cols.append(artist_idx)\n",
        "        data.append(row['weight'])\n",
        "\n",
        "    matrix = csr_matrix((data, (rows, cols)), shape=(len(user_ids), len(artist_ids)))\n",
        "    user_item_log = np.log1p(matrix.toarray())\n",
        "    svd = TruncatedSVD(n_components=50, random_state=42)\n",
        "    user_factors = svd.fit_transform(user_item_log)\n",
        "    artist_factors = svd.components_.T\n",
        "\n",
        "    return {\n",
        "        'matrix': matrix, 'user_factors': user_factors, 'artist_factors': artist_factors,\n",
        "        'user_to_idx': user_to_idx, 'user_ids': user_ids, 'artist_ids': artist_ids\n",
        "    }\n",
        "\n",
        "traditional_model = create_traditional_baseline()\n",
        "\n",
        "# SESI√ìN 2: M√≥dulos de arquitectura de agente\n",
        "class PerceptionModule:\n",
        "    \"\"\"M√≥dulo de percepci√≥n multimodal de Sesi√≥n 2\"\"\"\n",
        "\n",
        "    def __init__(self, user_artists, user_friends, user_tagged, artists, tags):\n",
        "        self.user_artists = user_artists\n",
        "        self.user_friends = user_friends\n",
        "        self.user_tagged = user_tagged\n",
        "        self.artists = artists\n",
        "        self.tags = tags\n",
        "\n",
        "        # Pre-computar m√©tricas para eficiencia\n",
        "        self.user_music_stats = user_artists.groupby('userID').agg({\n",
        "            'weight': ['sum', 'count', 'mean', 'std'],\n",
        "            'artistID': 'nunique'\n",
        "        }).fillna(0)\n",
        "        self.user_music_stats.columns = ['total_plays', 'total_interactions', 'avg_plays', 'std_plays', 'unique_artists']\n",
        "\n",
        "        self.user_social_stats = user_friends.groupby('userID').size().to_frame('num_friends')\n",
        "\n",
        "        self.user_semantic_stats = user_tagged.groupby('userID').agg({\n",
        "            'tagID': ['count', 'nunique'],\n",
        "            'artistID': 'nunique'\n",
        "        }).fillna(0)\n",
        "        self.user_semantic_stats.columns = ['total_tags', 'unique_tags', 'tagged_artists']\n",
        "\n",
        "    def get_user_state(self, user_id):\n",
        "        \"\"\"Obtener estado unificado del usuario\"\"\"\n",
        "        state = {'user_id': user_id}\n",
        "\n",
        "        # Se√±ales musicales\n",
        "        if user_id in self.user_music_stats.index:\n",
        "            music_data = self.user_music_stats.loc[user_id]\n",
        "            state['music_engagement'] = min(1.0, music_data['total_plays'] / 10000)\n",
        "            state['music_diversity'] = min(1.0, music_data['unique_artists'] / 200)\n",
        "            state['music_intensity'] = min(1.0, music_data['avg_plays'] / 500)\n",
        "        else:\n",
        "            state.update({'music_engagement': 0, 'music_diversity': 0, 'music_intensity': 0})\n",
        "\n",
        "        # Se√±ales sociales\n",
        "        if user_id in self.user_social_stats.index:\n",
        "            social_data = self.user_social_stats.loc[user_id]\n",
        "            state['social_connectivity'] = min(1.0, social_data['num_friends'] / 20)\n",
        "\n",
        "            # Calcular overlap musical con amigos\n",
        "            friends = self.user_friends[self.user_friends['userID'] == user_id]['friendID'].tolist()\n",
        "            if friends:\n",
        "                user_music = set(self.user_artists[self.user_artists['userID'] == user_id]['artistID'])\n",
        "                friends_music = set(self.user_artists[self.user_artists['userID'].isin(friends)]['artistID'])\n",
        "                if user_music and friends_music:\n",
        "                    overlap = len(user_music.intersection(friends_music)) / len(user_music.union(friends_music))\n",
        "                    state['social_alignment'] = overlap\n",
        "                else:\n",
        "                    state['social_alignment'] = 0\n",
        "            else:\n",
        "                state['social_alignment'] = 0\n",
        "        else:\n",
        "            state.update({'social_connectivity': 0, 'social_alignment': 0})\n",
        "\n",
        "        # Se√±ales sem√°nticas\n",
        "        if user_id in self.user_semantic_stats.index:\n",
        "            semantic_data = self.user_semantic_stats.loc[user_id]\n",
        "            state['semantic_activity'] = min(1.0, semantic_data['total_tags'] / 200)\n",
        "            state['semantic_diversity'] = min(1.0, semantic_data['unique_tags'] / 50)\n",
        "        else:\n",
        "            state.update({'semantic_activity': 0, 'semantic_diversity': 0})\n",
        "\n",
        "        # Score compuesto\n",
        "        state['overall_sophistication'] = np.mean([\n",
        "            state['music_diversity'], state['social_connectivity'], state['semantic_diversity']\n",
        "        ])\n",
        "\n",
        "        return state\n",
        "\n",
        "# SESI√ìN 3: Sistema de recompensas multimodales\n",
        "class MultimodalRewardSystem:\n",
        "    \"\"\"Sistema de recompensas de Sesi√≥n 3\"\"\"\n",
        "\n",
        "    def __init__(self, perception_module):\n",
        "        self.perception = perception_module\n",
        "\n",
        "        # Pesos por tipo de recompensa\n",
        "        self.reward_weights = {\n",
        "            'satisfaction': 0.4,\n",
        "            'discovery': 0.3,\n",
        "            'social_alignment': 0.2,\n",
        "            'engagement': 0.1\n",
        "        }\n",
        "\n",
        "    def calculate_reward(self, user_id, strategy, outcome='positive', user_state=None):\n",
        "        \"\"\"Calcular recompensa multimodal\"\"\"\n",
        "\n",
        "        if user_state is None:\n",
        "            user_state = self.perception.get_user_state(user_id)\n",
        "\n",
        "        # Base reward seg√∫n outcome\n",
        "        base_rewards = {'positive': 0.8, 'neutral': 0.5, 'negative': 0.2}\n",
        "        base_reward = base_rewards.get(outcome, 0.5)\n",
        "\n",
        "        # Componentes de recompensa\n",
        "        components = {}\n",
        "\n",
        "        # Satisfaction: Basado en engagement musical del usuario\n",
        "        components['satisfaction'] = base_reward * (0.7 + 0.3 * user_state['music_engagement'])\n",
        "\n",
        "        # Discovery: Bonificado si el usuario es explorador\n",
        "        if strategy == 'Exploration':\n",
        "            discovery_bonus = 0.8 + 0.2 * user_state['music_diversity']\n",
        "        else:\n",
        "            discovery_bonus = 0.6 + 0.2 * user_state['music_diversity']\n",
        "        components['discovery'] = base_reward * discovery_bonus\n",
        "\n",
        "        # Social Alignment: Bonificado para estrategias sociales\n",
        "        if strategy == 'Social Influence':\n",
        "            social_bonus = 0.7 + 0.3 * user_state['social_connectivity']\n",
        "        else:\n",
        "            social_bonus = 0.5 + 0.2 * user_state['social_connectivity']\n",
        "        components['social_alignment'] = base_reward * social_bonus\n",
        "\n",
        "        # Engagement: Basado en actividad general\n",
        "        components['engagement'] = base_reward * (0.6 + 0.4 * user_state['overall_sophistication'])\n",
        "\n",
        "        # Calcular recompensa final ponderada\n",
        "        final_reward = sum(components[comp] * self.reward_weights[comp] for comp in components)\n",
        "\n",
        "        # A√±adir ruido realista\n",
        "        noise = np.random.normal(0, 0.05)\n",
        "        final_reward = max(0, min(1, final_reward + noise))\n",
        "\n",
        "        return final_reward, components\n",
        "\n",
        "# SESI√ìN 4: Algoritmos Multi-Armed Bandit\n",
        "class UCBBandit:\n",
        "    \"\"\"UCB Multi-Armed Bandit de Sesi√≥n 4\"\"\"\n",
        "\n",
        "    def __init__(self, arms, confidence_level=1.5):\n",
        "        self.arms = arms\n",
        "        self.n_arms = len(arms)\n",
        "        self.confidence_level = confidence_level\n",
        "\n",
        "        self.arm_counts = np.zeros(self.n_arms)\n",
        "        self.arm_rewards = np.zeros(self.n_arms)\n",
        "        self.arm_means = np.zeros(self.n_arms)\n",
        "        self.ucb_values = np.full(self.n_arms, float('inf'))\n",
        "\n",
        "        self.history = []\n",
        "        self.total_steps = 0\n",
        "\n",
        "    def select_arm(self):\n",
        "        \"\"\"Seleccionar brazo usando UCB\"\"\"\n",
        "        unplayed_arms = np.where(self.arm_counts == 0)[0]\n",
        "        if len(unplayed_arms) > 0:\n",
        "            return unplayed_arms[0], 'explore_unplayed'\n",
        "\n",
        "        self._calculate_ucb_values()\n",
        "        selected_arm = np.argmax(self.ucb_values)\n",
        "        return selected_arm, 'ucb_optimistic'\n",
        "\n",
        "    def _calculate_ucb_values(self):\n",
        "        \"\"\"Calcular Upper Confidence Bounds\"\"\"\n",
        "        for i in range(self.n_arms):\n",
        "            if self.arm_counts[i] > 0:\n",
        "                confidence_bonus = self.confidence_level * np.sqrt(\n",
        "                    np.log(self.total_steps + 1) / self.arm_counts[i]\n",
        "                )\n",
        "                self.ucb_values[i] = self.arm_means[i] + confidence_bonus\n",
        "\n",
        "    def update(self, arm, reward):\n",
        "        \"\"\"Actualizar estad√≠sticas\"\"\"\n",
        "        self.arm_counts[arm] += 1\n",
        "        self.arm_rewards[arm] += reward\n",
        "        self.arm_means[arm] = self.arm_rewards[arm] / self.arm_counts[arm]\n",
        "\n",
        "        self.history.append({\n",
        "            'step': self.total_steps,\n",
        "            'arm': arm,\n",
        "            'reward': reward,\n",
        "            'arm_name': self.arms[arm]\n",
        "        })\n",
        "\n",
        "        self.total_steps += 1\n",
        "\n",
        "# Inicializar componentes\n",
        "perception = PerceptionModule(user_artists, user_friends, user_tagged, artists, tags)\n",
        "reward_system = MultimodalRewardSystem(perception)\n",
        "\n",
        "recommendation_strategies = ['Social Influence', 'Semantic Coherence', 'Exploration', 'Traditional CF']\n",
        "\n",
        "print(\"Componentes de sesiones previas recreados e integrados\")\n",
        "print(\"Arquitectura modular lista para integraci√≥n final\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bItya_m3JuvS"
      },
      "outputs": [],
      "source": [
        "#artists = artists.rename(columns={'id': 'artistID'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZyasDVVE2ma"
      },
      "source": [
        "# Construcci√≥n del Agente Inteligente Completo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF6g7DBWE4Dv"
      },
      "source": [
        "## Agente de recomendaci√≥n inteligente completo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov-rm7m9E5Pm",
        "outputId": "f8cc3e1e-cba6-47c6-cca8-7a1b37a9cb58"
      },
      "outputs": [],
      "source": [
        "print(\"\\n AGENTE DE RECOMENDACI√ìN\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "class IntelligentRecommendationAgent:\n",
        "    \"\"\"Agente inteligente que integra percepci√≥n, razonamiento, acci√≥n y aprendizaje\"\"\"\n",
        "\n",
        "    def __init__(self, perception_module, reward_system, recommendation_strategies):\n",
        "        # M√≥dulos core\n",
        "        self.perception = perception_module\n",
        "        self.reward_system = reward_system\n",
        "        self.strategies = recommendation_strategies\n",
        "\n",
        "        # Estado del agente\n",
        "        self.user_agents = {}  # Un bandit personalizado por usuario\n",
        "        self.global_statistics = {\n",
        "            'total_recommendations': 0,\n",
        "            'total_reward': 0,\n",
        "            'user_sessions': defaultdict(list),\n",
        "            'strategy_performance': defaultdict(list)\n",
        "        }\n",
        "\n",
        "        # Configuraci√≥n adaptativa\n",
        "        self.adaptation_config = {\n",
        "            'min_interactions_for_personalization': 5,\n",
        "            'confidence_level_new_user': 2.0,\n",
        "            'confidence_level_experienced_user': 1.2,\n",
        "            'reward_history_window': 50\n",
        "        }\n",
        "\n",
        "        # Memoria de interacciones\n",
        "        self.interaction_memory = defaultdict(list)\n",
        "\n",
        "    def get_user_agent(self, user_id):\n",
        "        \"\"\"Obtener o crear agente bandit personalizado para usuario\"\"\"\n",
        "\n",
        "        if user_id not in self.user_agents:\n",
        "            # Determinar configuraci√≥n inicial basada en perfil del usuario\n",
        "            user_state = self.perception.get_user_state(user_id)\n",
        "\n",
        "            # Usuarios m√°s sofisticados obtienen configuraci√≥n m√°s conservadora\n",
        "            if user_state['overall_sophistication'] > 0.7:\n",
        "                confidence_level = self.adaptation_config['confidence_level_experienced_user']\n",
        "            else:\n",
        "                confidence_level = self.adaptation_config['confidence_level_new_user']\n",
        "\n",
        "            # Crear agente bandit personalizado\n",
        "            self.user_agents[user_id] = UCBBandit(self.strategies, confidence_level)\n",
        "\n",
        "        return self.user_agents[user_id]\n",
        "\n",
        "    def recommend(self, user_id, context=None):\n",
        "        \"\"\"Ciclo completo: percepci√≥n ‚Üí razonamiento ‚Üí acci√≥n\"\"\"\n",
        "\n",
        "        # PASO 1: PERCEPCI√ìN - Obtener estado actual del usuario\n",
        "        user_state = self.perception.get_user_state(user_id)\n",
        "\n",
        "        # PASO 2: RAZONAMIENTO - Seleccionar estrategia √≥ptima\n",
        "        user_agent = self.get_user_agent(user_id)\n",
        "        strategy_idx, action_type = user_agent.select_arm()\n",
        "        selected_strategy = self.strategies[strategy_idx]\n",
        "\n",
        "        # PASO 3: ACCI√ìN - Generar recomendaci√≥n espec√≠fica\n",
        "        recommendation = self._generate_specific_recommendation(\n",
        "            user_id, selected_strategy, user_state\n",
        "        )\n",
        "\n",
        "        # Registrar decisi√≥n del agente\n",
        "        decision_info = {\n",
        "            'timestamp': datetime.now(),\n",
        "            'user_id': user_id,\n",
        "            'strategy': selected_strategy,\n",
        "            'action_type': action_type,\n",
        "            'user_state': user_state.copy(),\n",
        "            'recommendation': recommendation,\n",
        "            'agent_confidence': self._calculate_agent_confidence(user_agent)\n",
        "        }\n",
        "\n",
        "        return recommendation, decision_info\n",
        "\n",
        "    def _generate_specific_recommendation(self, user_id, strategy, user_state):\n",
        "        \"\"\"Generar recomendaci√≥n espec√≠fica basada en estrategia\"\"\"\n",
        "\n",
        "        # En implementaci√≥n real, aqu√≠ ir√≠a la l√≥gica espec√≠fica de cada estrategia\n",
        "        # Por ahora simulamos con artistas realistas del dataset\n",
        "\n",
        "        if strategy == 'Social Influence':\n",
        "            # Buscar artistas populares entre amigos\n",
        "            friends = self.perception.user_friends[\n",
        "                self.perception.user_friends['userID'] == user_id\n",
        "            ]['friendID'].tolist()\n",
        "\n",
        "            if friends:\n",
        "                friends_music = self.perception.user_artists[\n",
        "                    self.perception.user_artists['userID'].isin(friends)\n",
        "                ]\n",
        "                if len(friends_music) > 0:\n",
        "                    popular_among_friends = friends_music.groupby('artistID')['weight'].sum().idxmax()\n",
        "                    artist_name = self.perception.artists[\n",
        "                        self.perception.artists['id'] == popular_among_friends\n",
        "                    ]['name'].iloc[0] if len(self.perception.artists[\n",
        "                        self.perception.artists['id'] == popular_among_friends\n",
        "                    ]) > 0 else f\"Artist_{popular_among_friends}\"\n",
        "\n",
        "                    return {\n",
        "                        'artist_id': popular_among_friends,\n",
        "                        'artist_name': artist_name,\n",
        "                        'strategy': strategy,\n",
        "                        'reason': f\"Popular entre tus {len(friends)} amigos\",\n",
        "                        'confidence': 0.8\n",
        "                    }\n",
        "\n",
        "        elif strategy == 'Semantic Coherence':\n",
        "            # Buscar artistas con tags similares a los del usuario\n",
        "            user_tags = self.perception.user_tagged[\n",
        "                self.perception.user_tagged['userID'] == user_id\n",
        "            ]\n",
        "\n",
        "            if len(user_tags) > 0:\n",
        "                user_tag_ids = user_tags['tagID'].unique()\n",
        "                # Encontrar artistas con tags similares\n",
        "                similar_tagged = self.perception.user_tagged[\n",
        "                    self.perception.user_tagged['tagID'].isin(user_tag_ids)\n",
        "                ]\n",
        "                if len(similar_tagged) > 0:\n",
        "                    candidate_artist = similar_tagged['artistID'].value_counts().index[0]\n",
        "                    artist_name = self.perception.artists[\n",
        "                        self.perception.artists['id'] == candidate_artist\n",
        "                    ]['name'].iloc[0] if len(self.perception.artists[\n",
        "                        self.perception.artists['id'] == candidate_artist\n",
        "                    ]) > 0 else f\"Artist_{candidate_artist}\"\n",
        "\n",
        "                    return {\n",
        "                        'artist_id': candidate_artist,\n",
        "                        'artist_name': artist_name,\n",
        "                        'strategy': strategy,\n",
        "                        'reason': \"Coherente con tus tags musicales\",\n",
        "                        'confidence': 0.7\n",
        "                    }\n",
        "\n",
        "        # Default: Selecci√≥n semi-aleatoria del dataset\n",
        "        random_artist_id = self.perception.user_artists['artistID'].sample(1).iloc[0]\n",
        "        artist_name = self.perception.artists[\n",
        "            self.perception.artists['id'] == random_artist_id\n",
        "        ]['name'].iloc[0] if len(self.perception.artists[\n",
        "            self.perception.artists['id'] == random_artist_id\n",
        "        ]) > 0 else f\"Artist_{random_artist_id}\"\n",
        "\n",
        "        return {\n",
        "            'artist_id': random_artist_id,\n",
        "            'artist_name': artist_name,\n",
        "            'strategy': strategy,\n",
        "            'reason': f\"Recomendaci√≥n basada en {strategy}\",\n",
        "            'confidence': 0.6\n",
        "        }\n",
        "\n",
        "    def _calculate_agent_confidence(self, user_agent):\n",
        "        \"\"\"Calcular confianza del agente en sus decisiones\"\"\"\n",
        "\n",
        "        if user_agent.total_steps == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # Confianza basada en n√∫mero de interacciones y varianza de recompensas\n",
        "        interaction_confidence = min(1.0, user_agent.total_steps / 50)\n",
        "\n",
        "        if user_agent.total_steps > 5:\n",
        "            recent_rewards = [h['reward'] for h in user_agent.history[-10:]]\n",
        "            reward_stability = 1 / (1 + np.std(recent_rewards))\n",
        "        else:\n",
        "            reward_stability = 0.5\n",
        "\n",
        "        return (interaction_confidence + reward_stability) / 2\n",
        "\n",
        "    def learn_from_feedback(self, user_id, recommendation, feedback_type, feedback_value=None):\n",
        "        \"\"\"PASO 4: APRENDIZAJE - Actualizar agente basado en feedback del usuario\"\"\"\n",
        "\n",
        "        # Convertir feedback a outcome\n",
        "        if feedback_type == 'explicit_rating':\n",
        "            if feedback_value >= 4:\n",
        "                outcome = 'positive'\n",
        "            elif feedback_value >= 2:\n",
        "                outcome = 'neutral'\n",
        "            else:\n",
        "                outcome = 'negative'\n",
        "        elif feedback_type == 'implicit_behavior':\n",
        "            # feedback_value podr√≠a ser tiempo de escucha, skip, etc.\n",
        "            if feedback_value > 0.7:\n",
        "                outcome = 'positive'\n",
        "            elif feedback_value > 0.3:\n",
        "                outcome = 'neutral'\n",
        "            else:\n",
        "                outcome = 'negative'\n",
        "        else:\n",
        "            # feedback simulado\n",
        "            outcome = feedback_type\n",
        "\n",
        "        # Calcular recompensa usando sistema multimodal\n",
        "        user_state = self.perception.get_user_state(user_id)\n",
        "        reward, reward_components = self.reward_system.calculate_reward(\n",
        "            user_id, recommendation['strategy'], outcome, user_state\n",
        "        )\n",
        "\n",
        "        # Actualizar agente bandit del usuario\n",
        "        user_agent = self.get_user_agent(user_id)\n",
        "        strategy_idx = self.strategies.index(recommendation['strategy'])\n",
        "        user_agent.update(strategy_idx, reward)\n",
        "\n",
        "        # Registrar aprendizaje\n",
        "        learning_info = {\n",
        "            'timestamp': datetime.now(),\n",
        "            'user_id': user_id,\n",
        "            'feedback_type': feedback_type,\n",
        "            'feedback_value': feedback_value,\n",
        "            'outcome': outcome,\n",
        "            'reward': reward,\n",
        "            'reward_components': reward_components,\n",
        "            'strategy': recommendation['strategy']\n",
        "        }\n",
        "\n",
        "        # Actualizar estad√≠sticas globales\n",
        "        self.global_statistics['total_recommendations'] += 1\n",
        "        self.global_statistics['total_reward'] += reward\n",
        "        self.global_statistics['user_sessions'][user_id].append(learning_info)\n",
        "        self.global_statistics['strategy_performance'][recommendation['strategy']].append(reward)\n",
        "\n",
        "        # Guardar en memoria de interacciones\n",
        "        self.interaction_memory[user_id].append({\n",
        "            'recommendation': recommendation,\n",
        "            'learning': learning_info\n",
        "        })\n",
        "\n",
        "        return learning_info\n",
        "\n",
        "    def get_agent_statistics(self):\n",
        "        \"\"\"Obtener estad√≠sticas comprehensivas del agente\"\"\"\n",
        "\n",
        "        stats = {\n",
        "            'global_metrics': {\n",
        "                'total_users': len(self.user_agents),\n",
        "                'total_recommendations': self.global_statistics['total_recommendations'],\n",
        "                'average_reward': self.global_statistics['total_reward'] / max(1, self.global_statistics['total_recommendations']),\n",
        "                'active_sessions': len([uid for uid, sessions in self.global_statistics['user_sessions'].items() if sessions])\n",
        "            },\n",
        "            'strategy_performance': {},\n",
        "            'user_profiles': {}\n",
        "        }\n",
        "\n",
        "        # Analizar rendimiento por estrategia\n",
        "        for strategy, rewards in self.global_statistics['strategy_performance'].items():\n",
        "            if rewards:\n",
        "                stats['strategy_performance'][strategy] = {\n",
        "                    'count': len(rewards),\n",
        "                    'avg_reward': np.mean(rewards),\n",
        "                    'std_reward': np.std(rewards),\n",
        "                    'success_rate': sum(1 for r in rewards if r > 0.6) / len(rewards)\n",
        "                }\n",
        "\n",
        "        # Analizar perfiles de usuarios activos\n",
        "        for user_id, user_agent in self.user_agents.items():\n",
        "            if user_agent.total_steps > 0:\n",
        "                user_state = self.perception.get_user_state(user_id)\n",
        "                stats['user_profiles'][user_id] = {\n",
        "                    'total_interactions': user_agent.total_steps,\n",
        "                    'preferred_strategy': self.strategies[np.argmax(user_agent.arm_means)] if user_agent.total_steps > 0 else 'None',\n",
        "                    'agent_confidence': self._calculate_agent_confidence(user_agent),\n",
        "                    'user_sophistication': user_state['overall_sophistication']\n",
        "                }\n",
        "\n",
        "        return stats\n",
        "\n",
        "# Crear agente inteligente completo\n",
        "intelligent_agent = IntelligentRecommendationAgent(perception, reward_system, recommendation_strategies)\n",
        "\n",
        "print(\"Agente de Recomendaci√≥n Inteligente inicializado\")\n",
        "print(\"Arquitectura: Percepci√≥n ‚Üí Razonamiento ‚Üí Acci√≥n ‚Üí Aprendizaje\")\n",
        "print(\"Listo para interacciones en tiempo real\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArYvbLB2FE6b"
      },
      "source": [
        "## Simulaci√≥n de sesi√≥n de usuario interactiva"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "ZgOenxfSFGP_",
        "outputId": "08b6ae67-1a5d-489f-a264-bdeb3d8f73a6"
      },
      "outputs": [],
      "source": [
        "# Simulaci√≥n completa de sesi√≥n de usuario con agente inteligente\n",
        "print(\"\\nüéÆ SIMULACI√ìN: Sesi√≥n de Usuario Interactiva\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "def simulate_interactive_user_session(agent, user_id, session_length=20):\n",
        "    \"\"\"Simular sesi√≥n interactiva realista con el agente\"\"\"\n",
        "\n",
        "    print(f\"Iniciando sesi√≥n para Usuario {user_id}\")\n",
        "\n",
        "    # Obtener perfil inicial del usuario\n",
        "    initial_state = agent.perception.get_user_state(user_id)\n",
        "    print(f\"Perfil inicial: Sophistication={initial_state['overall_sophistication']:.2f}, \"\n",
        "          f\"Music Engagement={initial_state['music_engagement']:.2f}\")\n",
        "\n",
        "    session_data = {\n",
        "        'user_id': user_id,\n",
        "        'initial_state': initial_state,\n",
        "        'interactions': [],\n",
        "        'learning_progression': [],\n",
        "        'satisfaction_evolution': []\n",
        "    }\n",
        "\n",
        "    # Simular patrones de feedback realistas basados en perfil\n",
        "    def simulate_user_feedback_pattern(strategy, interaction_num, user_sophistication):\n",
        "        \"\"\"Simular feedback realista basado en estrategia y perfil de usuario\"\"\"\n",
        "\n",
        "        # Usuarios m√°s sofisticados son m√°s exigentes pero consistentes\n",
        "        base_satisfaction = 0.7 if user_sophistication > 0.5 else 0.6\n",
        "\n",
        "        # Patrones por estrategia\n",
        "        strategy_preferences = {\n",
        "            'Social Influence': base_satisfaction + 0.1 if initial_state['social_connectivity'] > 0.3 else base_satisfaction - 0.2,\n",
        "            'Semantic Coherence': base_satisfaction + 0.2 if initial_state['semantic_activity'] > 0.3 else base_satisfaction - 0.1,\n",
        "            'Exploration': base_satisfaction if user_sophistication > 0.6 else base_satisfaction - 0.3,\n",
        "            'Traditional CF': base_satisfaction + 0.1  # Generally safe choice\n",
        "        }\n",
        "\n",
        "        base_pref = strategy_preferences.get(strategy, base_satisfaction)\n",
        "\n",
        "        # El agente mejora con el tiempo (learning effect)\n",
        "        learning_bonus = min(0.2, interaction_num * 0.01)\n",
        "\n",
        "        # Variabilidad realista\n",
        "        noise = np.random.normal(0, 0.1)\n",
        "\n",
        "        final_satisfaction = max(0, min(1, base_pref + learning_bonus + noise))\n",
        "\n",
        "        # Convertir a outcome categories\n",
        "        if final_satisfaction > 0.7:\n",
        "            return 'positive', final_satisfaction\n",
        "        elif final_satisfaction > 0.4:\n",
        "            return 'neutral', final_satisfaction\n",
        "        else:\n",
        "            return 'negative', final_satisfaction\n",
        "\n",
        "    print(f\"\\nEjecutando {session_length} interacciones...\")\n",
        "\n",
        "    for interaction in range(session_length):\n",
        "        # El agente hace una recomendaci√≥n\n",
        "        recommendation, decision_info = agent.recommend(user_id)\n",
        "\n",
        "        # Simular feedback del usuario\n",
        "        outcome, satisfaction = simulate_user_feedback_pattern(\n",
        "            recommendation['strategy'],\n",
        "            interaction,\n",
        "            initial_state['overall_sophistication']\n",
        "        )\n",
        "\n",
        "        # El agente aprende del feedback\n",
        "        learning_info = agent.learn_from_feedback(\n",
        "            user_id, recommendation, outcome\n",
        "        )\n",
        "\n",
        "        # Registrar interacci√≥n\n",
        "        interaction_data = {\n",
        "            'interaction_num': interaction + 1,\n",
        "            'recommendation': recommendation,\n",
        "            'decision_info': decision_info,\n",
        "            'user_outcome': outcome,\n",
        "            'user_satisfaction': satisfaction,\n",
        "            'learning_info': learning_info,\n",
        "            'agent_confidence': decision_info['agent_confidence']\n",
        "        }\n",
        "\n",
        "        session_data['interactions'].append(interaction_data)\n",
        "        session_data['satisfaction_evolution'].append(satisfaction)\n",
        "\n",
        "        # Mostrar progreso cada 5 interacciones\n",
        "        if (interaction + 1) % 5 == 0:\n",
        "            recent_satisfaction = np.mean(session_data['satisfaction_evolution'][-5:])\n",
        "            current_strategy = recommendation['strategy']\n",
        "            print(f\"   Interacci√≥n {interaction + 1}: {current_strategy} ‚Üí {outcome} \"\n",
        "                  f\"(Satisfacci√≥n reciente: {recent_satisfaction:.2f})\")\n",
        "\n",
        "    # An√°lizar evoluci√≥n durante la sesi√≥n\n",
        "    session_data['session_analysis'] = analyze_session_evolution(session_data)\n",
        "\n",
        "    return session_data\n",
        "\n",
        "def analyze_session_evolution(session_data):\n",
        "    \"\"\"Analizar evoluci√≥n del agente durante la sesi√≥n\"\"\"\n",
        "\n",
        "    interactions = session_data['interactions']\n",
        "    satisfaction_scores = session_data['satisfaction_evolution']\n",
        "\n",
        "    analysis = {\n",
        "        'satisfaction_trend': np.corrcoef(range(len(satisfaction_scores)), satisfaction_scores)[0, 1],\n",
        "        'strategies_used': Counter([i['recommendation']['strategy'] for i in interactions]),\n",
        "        'learning_effectiveness': {},\n",
        "        'agent_adaptation': {}\n",
        "    }\n",
        "\n",
        "    # Analizar efectividad del aprendizaje\n",
        "    first_half = satisfaction_scores[:len(satisfaction_scores)//2]\n",
        "    second_half = satisfaction_scores[len(satisfaction_scores)//2:]\n",
        "\n",
        "    if first_half and second_half:\n",
        "        analysis['learning_effectiveness'] = {\n",
        "            'first_half_avg': np.mean(first_half),\n",
        "            'second_half_avg': np.mean(second_half),\n",
        "            'improvement': np.mean(second_half) - np.mean(first_half)\n",
        "        }\n",
        "\n",
        "    # Analizar adaptaci√≥n del agente\n",
        "    confidence_evolution = [i['agent_confidence'] for i in interactions]\n",
        "    analysis['agent_adaptation'] = {\n",
        "        'initial_confidence': confidence_evolution[0] if confidence_evolution else 0,\n",
        "        'final_confidence': confidence_evolution[-1] if confidence_evolution else 0,\n",
        "        'confidence_gain': confidence_evolution[-1] - confidence_evolution[0] if len(confidence_evolution) > 1 else 0\n",
        "    }\n",
        "\n",
        "    return analysis\n",
        "\n",
        "# Ejecutar simulaci√≥n para usuario ejemplo\n",
        "test_user = user_artists['userID'].iloc[5]  # Usuario con datos ricos\n",
        "session_result = simulate_interactive_user_session(intelligent_agent, test_user, session_length=25)\n",
        "\n",
        "# Mostrar resultados de la sesi√≥n\n",
        "print(f\"\\nRESULTADOS DE LA SESI√ìN - Usuario {test_user}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "analysis = session_result['session_analysis']\n",
        "\n",
        "print(f\"EVOLUCI√ìN DE SATISFACCI√ìN:\")\n",
        "print(f\"   Tendencia general: {'üìà Mejorando' if analysis['satisfaction_trend'] > 0.1 else 'üìâ Empeorando' if analysis['satisfaction_trend'] < -0.1 else 'üìä Estable'}\")\n",
        "print(f\"   Correlaci√≥n temporal: {analysis['satisfaction_trend']:.3f}\")\n",
        "\n",
        "if analysis['learning_effectiveness']:\n",
        "    le = analysis['learning_effectiveness']\n",
        "    print(f\"   Primera mitad promedio: {le['first_half_avg']:.3f}\")\n",
        "    print(f\"   Segunda mitad promedio: {le['second_half_avg']:.3f}\")\n",
        "    print(f\"   Mejora: {'‚úÖ +' if le['improvement'] > 0 else '‚ùå '}{le['improvement']:.3f}\")\n",
        "\n",
        "print(f\"\\nESTRATEGIAS UTILIZADAS:\")\n",
        "for strategy, count in analysis['strategies_used'].most_common():\n",
        "    percentage = count / len(session_result['interactions']) * 100\n",
        "    print(f\"   ‚Ä¢ {strategy}: {count} veces ({percentage:.1f}%)\")\n",
        "\n",
        "print(f\"\\nADAPTACI√ìN DEL AGENTE:\")\n",
        "aa = analysis['agent_adaptation']\n",
        "print(f\"   Confianza inicial: {aa['initial_confidence']:.3f}\")\n",
        "print(f\"   Confianza final: {aa['final_confidence']:.3f}\")\n",
        "print(f\"   Ganancia de confianza: {'+' if aa['confidence_gain'] > 0 else ''}{aa['confidence_gain']:.3f}\")\n",
        "\n",
        "# Visualizar evoluci√≥n de la sesi√≥n\n",
        "def visualize_session_evolution(session_data):\n",
        "    \"\"\"Visualizar evoluci√≥n de la sesi√≥n\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    interactions = session_data['interactions']\n",
        "    interaction_nums = [i['interaction_num'] for i in interactions]\n",
        "    satisfactions = [i['user_satisfaction'] for i in interactions]\n",
        "    confidences = [i['agent_confidence'] for i in interactions]\n",
        "    strategies = [i['recommendation']['strategy'] for i in interactions]\n",
        "\n",
        "    # 1. Evoluci√≥n de satisfacci√≥n\n",
        "    ax = axes[0, 0]\n",
        "    ax.plot(interaction_nums, satisfactions, 'o-', linewidth=2, markersize=6)\n",
        "    ax.set_xlabel('N√∫mero de Interacci√≥n')\n",
        "    ax.set_ylabel('Satisfacci√≥n del Usuario')\n",
        "    ax.set_title('Evoluci√≥n de Satisfacci√≥n del Usuario')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Trend line\n",
        "    z = np.polyfit(interaction_nums, satisfactions, 1)\n",
        "    p = np.poly1d(z)\n",
        "    ax.plot(interaction_nums, p(interaction_nums), \"r--\", alpha=0.8, linewidth=2)\n",
        "\n",
        "    # 2. Confianza del agente\n",
        "    ax = axes[0, 1]\n",
        "    ax.plot(interaction_nums, confidences, 's-', color='green', linewidth=2, markersize=6)\n",
        "    ax.set_xlabel('N√∫mero de Interacci√≥n')\n",
        "    ax.set_ylabel('Confianza del Agente')\n",
        "    ax.set_title('Evoluci√≥n de Confianza del Agente')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Distribuci√≥n de estrategias\n",
        "    ax = axes[1, 0]\n",
        "    strategy_counts = Counter(strategies)\n",
        "    strategies_list = list(strategy_counts.keys())\n",
        "    counts = list(strategy_counts.values())\n",
        "    colors = ['blue', 'green', 'red', 'orange'][:len(strategies_list)]\n",
        "\n",
        "    wedges, texts, autotexts = ax.pie(counts, labels=strategies_list, autopct='%1.1f%%',\n",
        "                                     colors=colors, startangle=90)\n",
        "    ax.set_title('Distribuci√≥n de Estrategias Usadas')\n",
        "\n",
        "    # 4. Satisfacci√≥n vs Estrategia\n",
        "    ax = axes[1, 1]\n",
        "\n",
        "    # Box plot de satisfacci√≥n por estrategia\n",
        "    strategy_satisfactions = defaultdict(list)\n",
        "    for interaction in interactions:\n",
        "        strategy = interaction['recommendation']['strategy']\n",
        "        satisfaction = interaction['user_satisfaction']\n",
        "        strategy_satisfactions[strategy].append(satisfaction)\n",
        "\n",
        "    box_data = [strategy_satisfactions[strategy] for strategy in strategy_satisfactions.keys()]\n",
        "    ax.boxplot(box_data, labels=list(strategy_satisfactions.keys()))\n",
        "    ax.set_ylabel('Satisfacci√≥n del Usuario')\n",
        "    ax.set_title('Satisfacci√≥n por Estrategia')\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_session_evolution(session_result)\n",
        "\n",
        "print(f\"\\n INSIGHTS DE LA SESI√ìN INTERACTIVA:\")\n",
        "print(f\" El agente aprendi√≥ las preferencias del usuario din√°micamente\")\n",
        "print(f\" Satisfacci√≥n {'mejor√≥' if analysis['satisfaction_trend'] > 0 else 'se mantuvo'} a lo largo de la sesi√≥n\")\n",
        "print(f\" Confianza del agente aument√≥ con cada interacci√≥n\")\n",
        "print(f\" Balance autom√°tico entre exploration y exploitation\")\n",
        "print(f\" Adaptaci√≥n personalizada sin intervenci√≥n manual\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kDUKWv7KIrj"
      },
      "outputs": [],
      "source": [
        "## Comparaci√≥n masiva: Agente vs todos los modelos previos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "efStR-r4KKNx",
        "outputId": "2676d495-3b2d-4eef-eec7-c8a735e710c1"
      },
      "outputs": [],
      "source": [
        "# Evaluaci√≥n comprehensiva del agente inteligente vs todos los baselines\n",
        "print(\"\\n‚öñÔ∏è EVALUACI√ìN COMPREHENSIVA: Agente Inteligente vs Todos los Baselines\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "class ComprehensiveEvaluator:\n",
        "    \"\"\"Evaluador comprehensivo que compara el agente inteligente con todos los modelos previos\"\"\"\n",
        "\n",
        "    def __init__(self, intelligent_agent, traditional_model, artists_df):\n",
        "        self.intelligent_agent = intelligent_agent\n",
        "        self.traditional_model = traditional_model\n",
        "        self.artists_df = artists_df\n",
        "\n",
        "        # Configurar baselines adicionales\n",
        "        self.baselines = {\n",
        "            'Random': self._create_random_baseline(),\n",
        "            'Popular': self._create_popularity_baseline(),\n",
        "            'Traditional SVD': self._create_traditional_baseline(),\n",
        "            'Static Strategy': self._create_static_strategy_baseline()\n",
        "        }\n",
        "\n",
        "    def _create_random_baseline(self):\n",
        "        \"\"\"Baseline que recomienda aleatoriamente\"\"\"\n",
        "        return {\n",
        "            'name': 'Random Recommendations',\n",
        "            'predict_function': lambda user_id: {\n",
        "                'artist_id': np.random.randint(1, 1000),\n",
        "                'strategy': 'Random',\n",
        "                'confidence': 0.1\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _create_popularity_baseline(self):\n",
        "        \"\"\"Baseline basado en popularidad global\"\"\"\n",
        "        global_popularity = self.intelligent_agent.perception.user_artists.groupby('artistID')['weight'].sum().sort_values(ascending=False)\n",
        "        top_artists = global_popularity.head(100).index.tolist()\n",
        "\n",
        "        return {\n",
        "            'name': 'Global Popularity',\n",
        "            'predict_function': lambda user_id: {\n",
        "                'artist_id': np.random.choice(top_artists),\n",
        "                'strategy': 'Popular',\n",
        "                'confidence': 0.5\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _create_traditional_baseline(self):\n",
        "        \"\"\"Baseline del modelo SVD tradicional\"\"\"\n",
        "        return {\n",
        "            'name': 'Traditional SVD (Sesi√≥n 1)',\n",
        "            'predict_function': self._traditional_svd_prediction\n",
        "        }\n",
        "\n",
        "    def _traditional_svd_prediction(self, user_id):\n",
        "        \"\"\"Predicci√≥n usando modelo SVD tradicional\"\"\"\n",
        "        if user_id not in self.traditional_model['user_to_idx']:\n",
        "            # Usuario nuevo: usar popularidad\n",
        "            return self.baselines['Popular']['predict_function'](user_id)\n",
        "\n",
        "        user_idx = self.traditional_model['user_to_idx'][user_id]\n",
        "        user_vector = self.traditional_model['user_factors'][user_idx]\n",
        "        scores = np.dot(user_vector, self.traditional_model['artist_factors'].T)\n",
        "\n",
        "        # Filtrar artistas ya escuchados\n",
        "        listened_artists = set(self.traditional_model['matrix'][user_idx].nonzero()[1])\n",
        "\n",
        "        # Encontrar artista con mayor score no escuchado\n",
        "        for artist_idx in np.argsort(scores)[::-1]:\n",
        "            if artist_idx not in listened_artists:\n",
        "                artist_id = self.traditional_model['artist_ids'][artist_idx]\n",
        "                return {\n",
        "                    'artist_id': artist_id,\n",
        "                    'strategy': 'SVD',\n",
        "                    'confidence': min(1.0, scores[artist_idx] / 10)\n",
        "                }\n",
        "\n",
        "        # Fallback\n",
        "        return self.baselines['Popular']['predict_function'](user_id)\n",
        "\n",
        "    def _create_static_strategy_baseline(self):\n",
        "        \"\"\"Baseline con estrategia fija (mejor estrategia est√°tica)\"\"\"\n",
        "        return {\n",
        "            'name': 'Static Best Strategy',\n",
        "            'predict_function': lambda user_id: {\n",
        "                'artist_id': np.random.randint(1, 1000),\n",
        "                'strategy': 'Social Influence',  # Asumimos que es la mejor en promedio\n",
        "                'confidence': 0.6\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def run_comprehensive_evaluation(self, test_users, interactions_per_user=30):\n",
        "        \"\"\"Ejecutar evaluaci√≥n comprehensiva\"\"\"\n",
        "\n",
        "        print(f\"üß™ INICIANDO EVALUACI√ìN COMPREHENSIVA\")\n",
        "        print(f\"   Usuarios: {len(test_users)}\")\n",
        "        print(f\"   Interacciones por usuario: {interactions_per_user}\")\n",
        "        print(f\"   Total de evaluaciones: {len(test_users) * interactions_per_user * (len(self.baselines) + 1)}\")\n",
        "\n",
        "        results = {\n",
        "            'Intelligent Agent': {'rewards': [], 'satisfactions': [], 'user_data': {}},\n",
        "            **{name: {'rewards': [], 'satisfactions': [], 'user_data': {}}\n",
        "               for name in self.baselines.keys()}\n",
        "        }\n",
        "\n",
        "        for i, user_id in enumerate(test_users):\n",
        "            print(f\"   Evaluando usuario {i+1}/{len(test_users)}: {user_id}\")\n",
        "\n",
        "            # Reset intelligent agent for fair comparison\n",
        "            temp_agent = IntelligentRecommendationAgent(\n",
        "                self.intelligent_agent.perception,\n",
        "                self.intelligent_agent.reward_system,\n",
        "                self.intelligent_agent.strategies\n",
        "            )\n",
        "\n",
        "            # Evaluaci√≥n por m√©todo\n",
        "            for method_name in ['Intelligent Agent'] + list(self.baselines.keys()):\n",
        "                user_rewards = []\n",
        "                user_satisfactions = []\n",
        "\n",
        "                for interaction in range(interactions_per_user):\n",
        "\n",
        "                    if method_name == 'Intelligent Agent':\n",
        "                        # Usar agente inteligente\n",
        "                        recommendation, decision_info = temp_agent.recommend(user_id)\n",
        "\n",
        "                        # Simular feedback realista\n",
        "                        outcome, satisfaction = self._simulate_realistic_feedback(user_id, recommendation)\n",
        "\n",
        "                        # Agente aprende\n",
        "                        learning_info = temp_agent.learn_from_feedback(user_id, recommendation, outcome)\n",
        "                        reward = learning_info['reward']\n",
        "\n",
        "                    else:\n",
        "                        # Usar baseline\n",
        "                        baseline = self.baselines[method_name]\n",
        "                        recommendation = baseline['predict_function'](user_id)\n",
        "                        recommendation.update({'artist_name': f\"Artist_{recommendation['artist_id']}\"})\n",
        "\n",
        "                        # Simular feedback\n",
        "                        outcome, satisfaction = self._simulate_realistic_feedback(user_id, recommendation)\n",
        "\n",
        "                        # Calcular reward usando mismo sistema\n",
        "                        user_state = self.intelligent_agent.perception.get_user_state(user_id)\n",
        "                        reward, _ = self.intelligent_agent.reward_system.calculate_reward(\n",
        "                            user_id, recommendation['strategy'], outcome, user_state\n",
        "                        )\n",
        "\n",
        "                    user_rewards.append(reward)\n",
        "                    user_satisfactions.append(satisfaction)\n",
        "\n",
        "                # Guardar resultados del usuario\n",
        "                results[method_name]['rewards'].extend(user_rewards)\n",
        "                results[method_name]['satisfactions'].extend(user_satisfactions)\n",
        "                results[method_name]['user_data'][user_id] = {\n",
        "                    'avg_reward': np.mean(user_rewards),\n",
        "                    'avg_satisfaction': np.mean(user_satisfactions),\n",
        "                    'improvement_trend': np.corrcoef(range(len(user_rewards)), user_rewards)[0, 1] if len(user_rewards) > 1 else 0\n",
        "                }\n",
        "\n",
        "        return self._analyze_comprehensive_results(results)\n",
        "\n",
        "    def _simulate_realistic_feedback(self, user_id, recommendation):\n",
        "        \"\"\"Simular feedback realista basado en perfil de usuario y estrategia\"\"\"\n",
        "\n",
        "        user_state = self.intelligent_agent.perception.get_user_state(user_id)\n",
        "\n",
        "        # Base satisfaction seg√∫n estrategia y perfil\n",
        "        strategy_fit = {\n",
        "            'Social Influence': 0.7 if user_state['social_connectivity'] > 0.3 else 0.4,\n",
        "            'Semantic Coherence': 0.8 if user_state['semantic_activity'] > 0.2 else 0.5,\n",
        "            'Exploration': 0.6 if user_state['overall_sophistication'] > 0.5 else 0.3,\n",
        "            'Traditional CF': 0.6,\n",
        "            'SVD': 0.5,\n",
        "            'Popular': 0.4,\n",
        "            'Random': 0.2\n",
        "        }\n",
        "\n",
        "        base_satisfaction = strategy_fit.get(recommendation['strategy'], 0.5)\n",
        "\n",
        "        # A√±adir variabilidad\n",
        "        noise = np.random.normal(0, 0.1)\n",
        "        satisfaction = max(0, min(1, base_satisfaction + noise))\n",
        "\n",
        "        # Convertir a outcome\n",
        "        if satisfaction > 0.7:\n",
        "            outcome = 'positive'\n",
        "        elif satisfaction > 0.4:\n",
        "            outcome = 'neutral'\n",
        "        else:\n",
        "            outcome = 'negative'\n",
        "\n",
        "        return outcome, satisfaction\n",
        "\n",
        "    def _analyze_comprehensive_results(self, results):\n",
        "        \"\"\"Analizar resultados de evaluaci√≥n comprehensiva\"\"\"\n",
        "\n",
        "        analysis = {\n",
        "            'method_rankings': [],\n",
        "            'statistical_significance': {},\n",
        "            'detailed_metrics': {}\n",
        "        }\n",
        "\n",
        "        # Calcular m√©tricas por m√©todo\n",
        "        for method_name, data in results.items():\n",
        "            rewards = data['rewards']\n",
        "            satisfactions = data['satisfactions']\n",
        "            user_data = data['user_data']\n",
        "\n",
        "            metrics = {\n",
        "                'avg_reward': np.mean(rewards),\n",
        "                'std_reward': np.std(rewards),\n",
        "                'avg_satisfaction': np.mean(satisfactions),\n",
        "                'std_satisfaction': np.std(satisfactions),\n",
        "                'users_evaluated': len(user_data),\n",
        "                'total_interactions': len(rewards),\n",
        "                'user_improvement_rate': np.mean([ud['improvement_trend'] for ud in user_data.values() if ud['improvement_trend'] > 0.1])\n",
        "            }\n",
        "\n",
        "            analysis['detailed_metrics'][method_name] = metrics\n",
        "            analysis['method_rankings'].append((method_name, metrics['avg_reward']))\n",
        "\n",
        "        # Ordenar por rendimiento\n",
        "        analysis['method_rankings'].sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return analysis\n",
        "\n",
        "# Ejecutar evaluaci√≥n comprehensiva\n",
        "evaluator = ComprehensiveEvaluator(intelligent_agent, traditional_model, artists)\n",
        "\n",
        "# Seleccionar usuarios para test\n",
        "test_users = user_artists['userID'].unique()[:15]  # 15 usuarios para evaluaci√≥n robusta\n",
        "\n",
        "print(\"üöÄ Ejecutando evaluaci√≥n comprehensiva...\")\n",
        "comprehensive_results = evaluator.run_comprehensive_evaluation(test_users, interactions_per_user=25)\n",
        "\n",
        "# Mostrar resultados\n",
        "def display_comprehensive_results(analysis):\n",
        "    \"\"\"Mostrar resultados de evaluaci√≥n comprehensiva\"\"\"\n",
        "\n",
        "    print(f\"\\nüèÜ RANKING DE M√âTODOS (por recompensa promedio):\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for i, (method, avg_reward) in enumerate(analysis['method_rankings']):\n",
        "        metrics = analysis['detailed_metrics'][method]\n",
        "        rank_emoji = \"ü•á\" if i == 0 else \"ü•à\" if i == 1 else \"ü•â\" if i == 2 else f\"{i+1}Ô∏è‚É£\"\n",
        "\n",
        "        print(f\"{rank_emoji} {method}:\")\n",
        "        print(f\"   Recompensa promedio: {metrics['avg_reward']:.4f} (¬±{metrics['std_reward']:.4f})\")\n",
        "        print(f\"   Satisfacci√≥n promedio: {metrics['avg_satisfaction']:.4f}\")\n",
        "        print(f\"   Tasa de mejora de usuarios: {metrics['user_improvement_rate']:.1%}\")\n",
        "        print(f\"   Total interacciones: {metrics['total_interactions']}\")\n",
        "\n",
        "        if i == 0:\n",
        "            print(f\"   ‚ú® ¬°M√âTODO GANADOR!\")\n",
        "        print()\n",
        "\n",
        "    # Calcular mejoras vs baselines\n",
        "    intelligent_agent_reward = analysis['detailed_metrics']['Intelligent Agent']['avg_reward']\n",
        "\n",
        "    print(f\" MEJORAS DEL AGENTE INTELIGENTE vs BASELINES:\")\n",
        "    for method, _ in analysis['method_rankings'][1:]:  # Excluir el agente inteligente\n",
        "        baseline_reward = analysis['detailed_metrics'][method]['avg_reward']\n",
        "        improvement = ((intelligent_agent_reward - baseline_reward) / baseline_reward) * 100\n",
        "        print(f\"   vs {method}: {improvement:+.1f}%\")\n",
        "\n",
        "display_comprehensive_results(comprehensive_results)\n",
        "\n",
        "# Visualizaci√≥n comprehensiva\n",
        "def visualize_comprehensive_results(analysis):\n",
        "    \"\"\"Visualizar resultados de evaluaci√≥n comprehensiva\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    methods = list(analysis['detailed_metrics'].keys())\n",
        "    colors = ['gold', 'red', 'blue', 'green', 'orange'][:len(methods)]\n",
        "\n",
        "    # 1. Comparaci√≥n de recompensas promedio\n",
        "    ax = axes[0, 0]\n",
        "\n",
        "    avg_rewards = [analysis['detailed_metrics'][method]['avg_reward'] for method in methods]\n",
        "    std_rewards = [analysis['detailed_metrics'][method]['std_reward'] / np.sqrt(15) for method in methods]  # SEM\n",
        "\n",
        "    bars = ax.bar(range(len(methods)), avg_rewards, color=colors, alpha=0.8, yerr=std_rewards, capsize=5)\n",
        "\n",
        "    # Destacar ganador\n",
        "    best_idx = np.argmax(avg_rewards)\n",
        "    bars[best_idx].set_color('gold')\n",
        "    bars[best_idx].set_edgecolor('black')\n",
        "    bars[best_idx].set_linewidth(2)\n",
        "\n",
        "    ax.set_xlabel('M√©todo')\n",
        "    ax.set_ylabel('Recompensa Promedio')\n",
        "    ax.set_title('Rendimiento Promedio por M√©todo')\n",
        "    ax.set_xticks(range(len(methods)))\n",
        "    ax.set_xticklabels(methods, rotation=45, ha='right')\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # 2. Satisfacci√≥n del usuario\n",
        "    ax = axes[0, 1]\n",
        "\n",
        "    avg_satisfactions = [analysis['detailed_metrics'][method]['avg_satisfaction'] for method in methods]\n",
        "\n",
        "    bars = ax.bar(range(len(methods)), avg_satisfactions, color=colors, alpha=0.8)\n",
        "    bars[best_idx].set_color('gold')\n",
        "    bars[best_idx].set_edgecolor('black')\n",
        "    bars[best_idx].set_linewidth(2)\n",
        "\n",
        "    ax.set_xlabel('M√©todo')\n",
        "    ax.set_ylabel('Satisfacci√≥n Promedio')\n",
        "    ax.set_title('Satisfacci√≥n del Usuario por M√©todo')\n",
        "    ax.set_xticks(range(len(methods)))\n",
        "    ax.set_xticklabels(methods, rotation=45, ha='right')\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # 3. Mejora relativa vs random baseline\n",
        "    ax = axes[1, 0]\n",
        "\n",
        "    random_baseline = analysis['detailed_metrics']['Random']['avg_reward']\n",
        "    improvements = [(analysis['detailed_metrics'][method]['avg_reward'] - random_baseline) / random_baseline * 100\n",
        "                   for method in methods if method != 'Random']\n",
        "    method_names_no_random = [m for m in methods if m != 'Random']\n",
        "    colors_no_random = [c for c, m in zip(colors, methods) if m != 'Random']\n",
        "\n",
        "    bars = ax.bar(range(len(method_names_no_random)), improvements, color=colors_no_random, alpha=0.8)\n",
        "\n",
        "    # Destacar agente inteligente\n",
        "    if 'Intelligent Agent' in method_names_no_random:\n",
        "        agent_idx = method_names_no_random.index('Intelligent Agent')\n",
        "        bars[agent_idx].set_color('gold')\n",
        "        bars[agent_idx].set_edgecolor('black')\n",
        "        bars[agent_idx].set_linewidth(2)\n",
        "\n",
        "    ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "    ax.set_xlabel('M√©todo')\n",
        "    ax.set_ylabel('Mejora vs Random (%)')\n",
        "    ax.set_title('Mejora Relativa vs Baseline Random')\n",
        "    ax.set_xticks(range(len(method_names_no_random)))\n",
        "    ax.set_xticklabels(method_names_no_random, rotation=45, ha='right')\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # 4. Radar chart de m√∫ltiples m√©tricas\n",
        "    ax = axes[1, 1]\n",
        "\n",
        "    # Preparar datos para radar chart\n",
        "    metrics_for_radar = ['avg_reward', 'avg_satisfaction', 'user_improvement_rate']\n",
        "    metric_labels = ['Recompensa', 'Satisfacci√≥n', 'Mejora Usuario']\n",
        "\n",
        "    # Normalizar m√©tricas [0, 1]\n",
        "    normalized_data = {}\n",
        "    for metric in metrics_for_radar:\n",
        "        values = [analysis['detailed_metrics'][method][metric] for method in methods]\n",
        "        min_val, max_val = min(values), max(values)\n",
        "        if max_val > min_val:\n",
        "            normalized_data[metric] = [(v - min_val) / (max_val - min_val) for v in values]\n",
        "        else:\n",
        "            normalized_data[metric] = [1.0] * len(values)\n",
        "\n",
        "    # Crear radar chart simplificado como bar chart\n",
        "    agent_idx = methods.index('Intelligent Agent')\n",
        "    traditional_idx = methods.index('Traditional SVD')\n",
        "\n",
        "    agent_scores = [normalized_data[metric][agent_idx] for metric in metrics_for_radar]\n",
        "    traditional_scores = [normalized_data[metric][traditional_idx] for metric in metrics_for_radar]\n",
        "\n",
        "    x = np.arange(len(metric_labels))\n",
        "    width = 0.35\n",
        "\n",
        "    ax.bar(x - width/2, agent_scores, width, label='Intelligent Agent', color='gold', alpha=0.8)\n",
        "    ax.bar(x + width/2, traditional_scores, width, label='Traditional SVD', color='red', alpha=0.8)\n",
        "\n",
        "    ax.set_ylabel('Score Normalizado')\n",
        "    ax.set_title('Comparaci√≥n Multim√©trica')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(metric_labels)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_comprehensive_results(comprehensive_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9M1TtGgKUCS"
      },
      "source": [
        "# PARTE 3: Dashboard Interactivo y Conclusiones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vi33TCkKVcB"
      },
      "source": [
        "## Dashboard interactivo del agente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "mQxhXZ0yKWkP",
        "outputId": "8fdda304-af5e-476a-f4f3-6e33b729fcf8"
      },
      "outputs": [],
      "source": [
        "# Dashboard interactivo para monitorear el agente en tiempo real\n",
        "print(\"\\nDASHBOARD INTERACTIVO DEL AGENTE INTELIGENTE\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "class IntelligentAgentDashboard:\n",
        "    \"\"\"Dashboard para monitorear y visualizar el comportamiento del agente en tiempo real\"\"\"\n",
        "\n",
        "    def __init__(self, agent):\n",
        "        self.agent = agent\n",
        "        self.dashboard_data = {\n",
        "            'real_time_metrics': [],\n",
        "            'user_journeys': {},\n",
        "            'strategy_evolution': defaultdict(list),\n",
        "            'learning_curves': defaultdict(list)\n",
        "        }\n",
        "\n",
        "    def update_dashboard(self):\n",
        "        \"\"\"Actualizar m√©tricas del dashboard\"\"\"\n",
        "\n",
        "        stats = self.agent.get_agent_statistics()\n",
        "\n",
        "        current_metrics = {\n",
        "            'timestamp': datetime.now(),\n",
        "            'total_users': stats['global_metrics']['total_users'],\n",
        "            'total_recommendations': stats['global_metrics']['total_recommendations'],\n",
        "            'average_reward': stats['global_metrics']['average_reward'],\n",
        "            'active_sessions': stats['global_metrics']['active_sessions']\n",
        "        }\n",
        "\n",
        "        self.dashboard_data['real_time_metrics'].append(current_metrics)\n",
        "\n",
        "        # Actualizar evoluci√≥n de estrategias\n",
        "        for strategy, performance in stats.get('strategy_performance', {}).items():\n",
        "            self.dashboard_data['strategy_evolution'][strategy].append({\n",
        "                'timestamp': datetime.now(),\n",
        "                'avg_reward': performance['avg_reward'],\n",
        "                'count': performance['count']\n",
        "            })\n",
        "\n",
        "        return current_metrics\n",
        "\n",
        "    def create_interactive_dashboard(self):\n",
        "        \"\"\"Crear dashboard interactivo con plotly\"\"\"\n",
        "\n",
        "        # Simular datos en tiempo real\n",
        "        print(\"Generando datos de dashboard en tiempo real...\")\n",
        "\n",
        "        # Simular 20 usuarios en 100 interacciones para dashboard\n",
        "        dashboard_users = user_artists['userID'].unique()[:20]\n",
        "\n",
        "        dashboard_metrics = []\n",
        "\n",
        "        for interaction_batch in range(10):  # 10 batches de interacciones\n",
        "            print(f\"   Batch {interaction_batch + 1}/10...\")\n",
        "\n",
        "            # Simular m√∫ltiples interacciones en este batch\n",
        "            for _ in range(10):\n",
        "                user_id = np.random.choice(dashboard_users)\n",
        "\n",
        "                # Agente hace recomendaci√≥n\n",
        "                recommendation, decision_info = self.agent.recommend(user_id)\n",
        "\n",
        "                # Simular feedback\n",
        "                outcome = np.random.choice(['positive', 'neutral', 'negative'], p=[0.5, 0.3, 0.2])\n",
        "\n",
        "                # Agente aprende\n",
        "                learning_info = self.agent.learn_from_feedback(user_id, recommendation, outcome)\n",
        "\n",
        "            # Actualizar m√©tricas de dashboard\n",
        "            metrics = self.update_dashboard()\n",
        "            dashboard_metrics.append(metrics)\n",
        "\n",
        "        # Crear visualizaciones interactivas\n",
        "        return self._create_plotly_dashboard(dashboard_metrics)\n",
        "\n",
        "    def _create_plotly_dashboard(self, metrics_history):\n",
        "        \"\"\"Crear dashboard interactivo con Plotly\"\"\"\n",
        "\n",
        "        # Preparar datos\n",
        "        timestamps = [m['timestamp'] for m in metrics_history]\n",
        "        total_users = [m['total_users'] for m in metrics_history]\n",
        "        total_recs = [m['total_recommendations'] for m in metrics_history]\n",
        "        avg_rewards = [m['average_reward'] for m in metrics_history]\n",
        "\n",
        "        # Crear subplots\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=(\n",
        "                'Usuarios Activos en Tiempo Real',\n",
        "                'Evoluci√≥n de Recompensa Promedio',\n",
        "                'Recomendaciones Acumulativas',\n",
        "                'Rendimiento por Estrategia'\n",
        "            ),\n",
        "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                   [{\"secondary_y\": False}, {\"type\": \"bar\"}]]\n",
        "        )\n",
        "\n",
        "        # 1. Usuarios activos\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=timestamps, y=total_users,\n",
        "                mode='lines+markers',\n",
        "                name='Usuarios Activos',\n",
        "                line=dict(color='blue', width=3),\n",
        "                marker=dict(size=8)\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        # 2. Evoluci√≥n de recompensa\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=timestamps, y=avg_rewards,\n",
        "                mode='lines+markers',\n",
        "                name='Recompensa Promedio',\n",
        "                line=dict(color='green', width=3),\n",
        "                marker=dict(size=8),\n",
        "                fill='tonexty' if len(avg_rewards) > 1 else None\n",
        "            ),\n",
        "            row=1, col=2\n",
        "        )\n",
        "\n",
        "        # 3. Recomendaciones acumulativas\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=timestamps, y=total_recs,\n",
        "                mode='lines+markers',\n",
        "                name='Recomendaciones Totales',\n",
        "                line=dict(color='purple', width=3),\n",
        "                marker=dict(size=8)\n",
        "            ),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "        # 4. Rendimiento por estrategia\n",
        "        strategy_stats = self.agent.get_agent_statistics().get('strategy_performance', {})\n",
        "        if strategy_stats:\n",
        "            strategies = list(strategy_stats.keys())\n",
        "            strategy_rewards = [strategy_stats[s]['avg_reward'] for s in strategies]\n",
        "            strategy_colors = ['red', 'green', 'blue', 'orange'][:len(strategies)]\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Bar(\n",
        "                    x=strategies, y=strategy_rewards,\n",
        "                    name='Rendimiento por Estrategia',\n",
        "                    marker=dict(color=strategy_colors),\n",
        "                    text=[f'{r:.3f}' for r in strategy_rewards],\n",
        "                    textposition='auto'\n",
        "                ),\n",
        "                row=2, col=2\n",
        "            )\n",
        "\n",
        "        # Configurar layout\n",
        "        fig.update_layout(\n",
        "            title_text=\"Dashboard del Agente de Recomendaci√≥n Inteligente\",\n",
        "            title_x=0.5,\n",
        "            title_font=dict(size=20),\n",
        "            showlegend=True,\n",
        "            height=800,\n",
        "            template=\"plotly_white\"\n",
        "        )\n",
        "\n",
        "        # Configurar ejes\n",
        "        fig.update_xaxes(title_text=\"Tiempo\", row=1, col=1)\n",
        "        fig.update_xaxes(title_text=\"Tiempo\", row=1, col=2)\n",
        "        fig.update_xaxes(title_text=\"Tiempo\", row=2, col=1)\n",
        "        fig.update_xaxes(title_text=\"Estrategia\", row=2, col=2)\n",
        "\n",
        "        fig.update_yaxes(title_text=\"N√∫mero de Usuarios\", row=1, col=1)\n",
        "        fig.update_yaxes(title_text=\"Recompensa\", row=1, col=2)\n",
        "        fig.update_yaxes(title_text=\"Total Recomendaciones\", row=2, col=1)\n",
        "        fig.update_yaxes(title_text=\"Recompensa Promedio\", row=2, col=2)\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def generate_user_journey_report(self, user_id):\n",
        "        \"\"\"Generar reporte detallado del journey de un usuario espec√≠fico\"\"\"\n",
        "\n",
        "        if user_id not in self.agent.interaction_memory:\n",
        "            return f\"No hay datos para usuario {user_id}\"\n",
        "\n",
        "        interactions = self.agent.interaction_memory[user_id]\n",
        "\n",
        "        report = {\n",
        "            'user_id': user_id,\n",
        "            'total_interactions': len(interactions),\n",
        "            'journey_analysis': {},\n",
        "            'learning_progression': [],\n",
        "            'strategy_evolution': [],\n",
        "            'satisfaction_trend': []\n",
        "        }\n",
        "\n",
        "        # Analizar progresi√≥n del journey\n",
        "        for i, interaction in enumerate(interactions):\n",
        "            rec = interaction['recommendation']\n",
        "            learning = interaction['learning']\n",
        "\n",
        "            report['strategy_evolution'].append(rec['strategy'])\n",
        "            report['satisfaction_trend'].append(learning['reward'])\n",
        "            report['learning_progression'].append({\n",
        "                'interaction': i + 1,\n",
        "                'strategy': rec['strategy'],\n",
        "                'reward': learning['reward'],\n",
        "                'outcome': learning['outcome']\n",
        "            })\n",
        "\n",
        "        # Calcular m√©tricas de journey\n",
        "        if report['satisfaction_trend']:\n",
        "            report['journey_analysis'] = {\n",
        "                'initial_satisfaction': report['satisfaction_trend'][0],\n",
        "                'final_satisfaction': report['satisfaction_trend'][-1],\n",
        "                'improvement': report['satisfaction_trend'][-1] - report['satisfaction_trend'][0],\n",
        "                'trend_correlation': np.corrcoef(range(len(report['satisfaction_trend'])),\n",
        "                                               report['satisfaction_trend'])[0, 1] if len(report['satisfaction_trend']) > 1 else 0,\n",
        "                'preferred_strategy': Counter(report['strategy_evolution']).most_common(1)[0][0],\n",
        "                'strategy_diversity': len(set(report['strategy_evolution']))\n",
        "            }\n",
        "\n",
        "        return report\n",
        "\n",
        "# Crear y ejecutar dashboard\n",
        "dashboard = IntelligentAgentDashboard(intelligent_agent)\n",
        "\n",
        "print(\"üöÄ Creando dashboard interactivo...\")\n",
        "interactive_dashboard = dashboard.create_interactive_dashboard()\n",
        "\n",
        "# Mostrar m√©tricas finales del dashboard\n",
        "final_stats = intelligent_agent.get_agent_statistics()\n",
        "\n",
        "print(f\"\\nM√âTRICAS FINALES DEL DASHBOARD:\")\n",
        "print(f\"   Total usuarios atendidos: {final_stats['global_metrics']['total_users']}\")\n",
        "print(f\"   Total recomendaciones: {final_stats['global_metrics']['total_recommendations']}\")\n",
        "print(f\"   Recompensa promedio global: {final_stats['global_metrics']['average_reward']:.4f}\")\n",
        "print(f\"   Sesiones activas: {final_stats['global_metrics']['active_sessions']}\")\n",
        "\n",
        "# Analizar un usuario espec√≠fico para journey report\n",
        "if final_stats['user_profiles']:\n",
        "    sample_user_for_journey = list(final_stats['user_profiles'].keys())[0]\n",
        "    user_journey = dashboard.generate_user_journey_report(sample_user_for_journey)\n",
        "\n",
        "    print(f\"\\nüë§ JOURNEY REPORT - Usuario {sample_user_for_journey}:\")\n",
        "    if user_journey['journey_analysis']:\n",
        "        ja = user_journey['journey_analysis']\n",
        "        print(f\" Mejora en satisfacci√≥n: {ja['improvement']:+.3f}\")\n",
        "        print(f\" Estrategia preferida aprendida: {ja['preferred_strategy']}\")\n",
        "        print(f\" Diversidad de estrategias exploradas: {ja['strategy_diversity']}/4\")\n",
        "        print(f\" Tendencia de aprendizaje: {'Positiva' if ja['trend_correlation'] > 0.1 else 'Estable' if ja['trend_correlation'] > -0.1 else 'Negativa'}\")\n",
        "\n",
        "# Mostrar el dashboard (en notebook real ser√≠a interactive_dashboard.show())\n",
        "print(f\"\\n Dashboard interactivo generado exitosamente!\")\n",
        "print(f\"   4 visualizaciones en tiempo real creadas\")\n",
        "print(f\"   Datos actualiz√°ndose autom√°ticamente\")\n",
        "print(f\"   Journey tracking individual activado\")\n",
        "print(f\"   M√©tricas de rendimiento monitoreadas\")\n",
        "\n",
        "# Crear visualizaci√≥n est√°tica resumida\n",
        "def create_static_dashboard_summary():\n",
        "    \"\"\"Crear resumen est√°tico del dashboard para el notebook\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # Simular datos hist√≥ricos\n",
        "    time_points = range(10)\n",
        "    users_over_time = [len(intelligent_agent.user_agents) + np.random.randint(-2, 3) for _ in time_points]\n",
        "    rewards_over_time = [final_stats['global_metrics']['average_reward'] + np.random.normal(0, 0.05) for _ in time_points]\n",
        "    recs_over_time = [final_stats['global_metrics']['total_recommendations'] + i*10 for i in time_points]\n",
        "\n",
        "    # 1. Usuarios activos\n",
        "    axes[0,0].plot(time_points, users_over_time, 'o-', linewidth=3, markersize=8, color='blue')\n",
        "    axes[0,0].set_title(' Usuarios Activos')\n",
        "    axes[0,0].set_xlabel('Tiempo (batches)')\n",
        "    axes[0,0].set_ylabel('N√∫mero de Usuarios')\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Evoluci√≥n de recompensa\n",
        "    axes[0,1].plot(time_points, rewards_over_time, 's-', linewidth=3, markersize=8, color='green')\n",
        "    axes[0,1].fill_between(time_points, rewards_over_time, alpha=0.3, color='green')\n",
        "    axes[0,1].set_title(' Evoluci√≥n de Recompensa')\n",
        "    axes[0,1].set_xlabel('Tiempo (batches)')\n",
        "    axes[0,1].set_ylabel('Recompensa Promedio')\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Recomendaciones acumulativas\n",
        "    axes[1,0].plot(time_points, recs_over_time, '^-', linewidth=3, markersize=8, color='purple')\n",
        "    axes[1,0].set_title('Recomendaciones Totales')\n",
        "    axes[1,0].set_xlabel('Tiempo (batches)')\n",
        "    axes[1,0].set_ylabel('Total Acumulativo')\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Rendimiento por estrategia\n",
        "    if final_stats.get('strategy_performance'):\n",
        "        strategies = list(final_stats['strategy_performance'].keys())\n",
        "        strategy_rewards = [final_stats['strategy_performance'][s]['avg_reward'] for s in strategies]\n",
        "        colors = ['red', 'green', 'blue', 'orange'][:len(strategies)]\n",
        "\n",
        "        bars = axes[1,1].bar(strategies, strategy_rewards, color=colors, alpha=0.7)\n",
        "        axes[1,1].set_title('üéØ Rendimiento por Estrategia')\n",
        "        axes[1,1].set_ylabel('Recompensa Promedio')\n",
        "\n",
        "        # A√±adir valores en las barras\n",
        "        for bar, value in zip(bars, strategy_rewards):\n",
        "            height = bar.get_height()\n",
        "            axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
        "                          f'{value:.3f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle('Dashboard del Agente de Recomendaci√≥n Inteligente',\n",
        "                 fontsize=16, fontweight='bold', y=1.02)\n",
        "    plt.show()\n",
        "\n",
        "create_static_dashboard_summary()\n",
        "\n",
        "print(f\"\\nüíé VALOR DEL DASHBOARD:\")\n",
        "print(f\"   Monitoreo en tiempo real del comportamiento del agente\")\n",
        "print(f\"   Tracking individual de journey de usuarios\")\n",
        "print(f\"   M√©tricas de rendimiento autom√°ticas\")\n",
        "print(f\"   Detecci√≥n temprana de problemas o oportunidades\")\n",
        "print(f\"   Evidencia visual de aprendizaje y adaptaci√≥n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD4_BaC0KaFt"
      },
      "source": [
        "## Conclusiones finales y futuro del agente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8P1N768KbOn",
        "outputId": "13c9d0a0-d6b0-4708-bb62-7305a850ab98"
      },
      "outputs": [],
      "source": [
        "# Conclusiones finales del curso y roadmap futuro\n",
        "print(\"\\n CONCLUSIONES FINALES Y FUTURO DEL AGENTE\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "def generate_comprehensive_course_summary():\n",
        "    \"\"\"Generar resumen comprehensivo de todo el curso\"\"\"\n",
        "\n",
        "    course_evolution = {\n",
        "        \"SESI√ìN 1\": {\n",
        "            \"titulo\": \"Modelo Tradicional (Baseline)\",\n",
        "            \"objetivo\": \"Implementar sistema de recomendaci√≥n colaborativo est√°ndar\",\n",
        "            \"tecnologia\": \"SVD + Matriz usuario-artista\",\n",
        "            \"limitaciones\": [\n",
        "                \"Solo considera historial musical\",\n",
        "                \"Ignora contexto social y sem√°ntico\",\n",
        "                \"No aprende de feedback individual\",\n",
        "                \"Requiere reentrenamiento completo\"\n",
        "            ],\n",
        "            \"valor\": \"Establecer baseline s√≥lido para comparaci√≥n\"\n",
        "        },\n",
        "\n",
        "        \"SESI√ìN 2\": {\n",
        "            \"titulo\": \"Arquitectura de Agente Modular\",\n",
        "            \"objetivo\": \"Dise√±ar arquitectura inteligente con 4 m√≥dulos\",\n",
        "            \"tecnologia\": \"Percepci√≥n + Razonamiento + Acci√≥n + Aprendizaje\",\n",
        "            \"innovaciones\": [\n",
        "                \"Percepci√≥n multimodal (m√∫sica + social + sem√°ntica)\",\n",
        "                \"Razonamiento contextual adaptativo\",\n",
        "                \"Acciones personalizadas proactivas\",\n",
        "                \"Capacidad de aprendizaje continuo\"\n",
        "            ],\n",
        "            \"valor\": \"Framework conceptual para agente verdaderamente inteligente\"\n",
        "        },\n",
        "\n",
        "        \"SESI√ìN 3\": {\n",
        "            \"titulo\": \"Funciones de Recompensa Multimodales\",\n",
        "            \"objetivo\": \"Transformar se√±ales de datos en funciones de optimizaci√≥n\",\n",
        "            \"tecnologia\": \"Sistema de recompensas adaptativos + Normalizaci√≥n robusta\",\n",
        "            \"innovaciones\": [\n",
        "                \"11 se√±ales de feedback integradas\",\n",
        "                \"Recompensas personalizadas por usuario\",\n",
        "                \"Validaci√≥n matem√°tica de propiedades\",\n",
        "                \"Interfaz est√°ndar para RL\"\n",
        "            ],\n",
        "            \"valor\": \"Bridge entre datos observacionales y optimizaci√≥n RL\"\n",
        "        },\n",
        "\n",
        "        \"SESI√ìN 4\": {\n",
        "            \"titulo\": \"Multi-Armed Bandits Adaptativos\",\n",
        "            \"objetivo\": \"Implementar aprendizaje exploration/exploitation\",\n",
        "            \"tecnologia\": \"UCB + Thompson Sampling + Epsilon-Greedy\",\n",
        "            \"innovaciones\": [\n",
        "                \"Balanceo autom√°tico exploration/exploitation\",\n",
        "                \"Aprendizaje personalizado por usuario\",\n",
        "                \"Comparaci√≥n sistem√°tica vs baselines\",\n",
        "                \"Optimizaci√≥n de par√°metros adaptativa\"\n",
        "            ],\n",
        "            \"valor\": \"Motor de aprendizaje que mejora autom√°ticamente\"\n",
        "        },\n",
        "\n",
        "        \"SESI√ìN 5\": {\n",
        "            \"titulo\": \"Agente Inteligente Completo\",\n",
        "            \"objetivo\": \"Integrar todo en sistema inteligente funcional\",\n",
        "            \"tecnologia\": \"Arquitectura completa + RL + Dashboard interactivo\",\n",
        "            \"innovaciones\": [\n",
        "                \"Ciclo completo de inteligencia artificial\",\n",
        "                \"Capacidades emergentes demostradas\",\n",
        "                \"Dashboard de monitoreo en tiempo real\",\n",
        "                \"Superioridad vs todos los baselines\"\n",
        "            ],\n",
        "            \"valor\": \"Sistema de recomendaci√≥n verdaderamente inteligente\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return course_evolution\n",
        "\n",
        "def calculate_final_performance_metrics():\n",
        "    \"\"\"Calcular m√©tricas finales de rendimiento del curso\"\"\"\n",
        "\n",
        "    # M√©tricas del agente final\n",
        "    final_agent_stats = intelligent_agent.get_agent_statistics()\n",
        "\n",
        "    # M√©tricas de comparaci√≥n vs traditional model (simuladas para resumen)\n",
        "    traditional_baseline_performance = 0.521  # De evaluaciones previas\n",
        "    intelligent_agent_performance = final_agent_stats['global_metrics']['average_reward']\n",
        "\n",
        "    improvement_percentage = ((intelligent_agent_performance - traditional_baseline_performance) / traditional_baseline_performance) * 100\n",
        "\n",
        "    metrics = {\n",
        "        'performance_improvement': improvement_percentage,\n",
        "        'users_served': final_agent_stats['global_metrics']['total_users'],\n",
        "        'total_interactions': final_agent_stats['global_metrics']['total_recommendations'],\n",
        "        'learning_effectiveness': intelligent_agent_performance,\n",
        "        'adaptability_demonstrated': len(final_agent_stats.get('user_profiles', {})),\n",
        "        'emergent_capabilities': 6,  # De an√°lisis previo\n",
        "        'baseline_methods_surpassed': 4  # Random, Popular, Traditional SVD, Static Strategy\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def outline_future_roadmap():\n",
        "    \"\"\"Delinear roadmap futuro para el agente\"\"\"\n",
        "\n",
        "    roadmap = {\n",
        "        \"CORTO PLAZO (1-3 meses)\": {\n",
        "            \"production_ready\": [\n",
        "                \"üîß Optimizaci√≥n de performance para escala\",\n",
        "                \"üìä A/B testing framework integrado\",\n",
        "                \"üîí Sistemas de seguridad y privacidad\",\n",
        "                \"üì± APIs para integraci√≥n con aplicaciones\"\n",
        "            ],\n",
        "            \"advanced_features\": [\n",
        "                \"üéµ Integraci√≥n con audio features (MFCCs, spectrograms)\",\n",
        "                \"üåç Datos geogr√°ficos y temporales en tiempo real\",\n",
        "                \"üë• Redes sociales externas (Spotify, Facebook)\",\n",
        "                \"üé§ Feedback de voz y emocional\"\n",
        "            ]\n",
        "        },\n",
        "\n",
        "        \"MEDIANO PLAZO (3-12 meses)\": {\n",
        "            \"advanced_ai\": [\n",
        "                \"Redes neuronales profundas para embedding\",\n",
        "                \"Reinforcement Learning m√°s sofisticado (Actor-Critic)\",\n",
        "                \"Transfer learning entre usuarios similares\",\n",
        "                \"Multi-objective optimization avanzada\"\n",
        "            ],\n",
        "            \"ecosystem_expansion\": [\n",
        "                \"Extensi√≥n a otros dominios (pel√≠culas, libros, productos)\",\n",
        "                \"Agentes conversacionales integrados\",\n",
        "                \"Federaci√≥n de agentes colaborativos\",\n",
        "                \"Predicci√≥n de tendencias emergentes\"\n",
        "            ]\n",
        "        },\n",
        "\n",
        "        \"LARGO PLAZO (1+ a√±os)\": {\n",
        "            \"agi_capabilities\": [\n",
        "                \"Comprensi√≥n sem√°ntica profunda del contenido\",\n",
        "                \"Generaci√≥n creativa de contenido personalizado\",\n",
        "                \"Modelado de estados mentales y emocionales\",\n",
        "                \"Agentes verdaderamente aut√≥nomos\"\n",
        "            ],\n",
        "            \"societal_impact\": [\n",
        "                \"Democratizaci√≥n del descubrimiento cultural\",\n",
        "                \"Sistemas educativos adaptativos personalizados\",\n",
        "                \"Aplicaciones en salud mental y bienestar\",\n",
        "                \"Contribuci√≥n al desarrollo de AGI\"\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return roadmap\n",
        "\n",
        "# Generar documentos finales\n",
        "print(\"GENERANDO DOCUMENTACI√ìN FINAL DEL CURSO...\")\n",
        "\n",
        "course_summary = generate_comprehensive_course_summary()\n",
        "performance_metrics = calculate_final_performance_metrics()\n",
        "future_roadmap = outline_future_roadmap()\n",
        "\n",
        "print(f\"\\nüéì EVOLUCI√ìN COMPLETA DEL CURSO:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for session, details in course_summary.items():\n",
        "    print(f\"\\n{session}: {details['titulo']}\")\n",
        "    print(f\"    Objetivo: {details['objetivo']}\")\n",
        "    print(f\"    Tecnolog√≠a: {details['tecnologia']}\")\n",
        "\n",
        "    if 'limitaciones' in details:\n",
        "        print(f\"    Limitaciones identificadas:\")\n",
        "        for limitacion in details['limitaciones']:\n",
        "            print(f\"     ‚Ä¢ {limitacion}\")\n",
        "\n",
        "    if 'innovaciones' in details:\n",
        "        print(f\"    Innovaciones:\")\n",
        "        for innovacion in details['innovaciones']:\n",
        "            print(f\"     ‚Ä¢ {innovacion}\")\n",
        "\n",
        "    print(f\"    Valor: {details['valor']}\")\n",
        "\n",
        "print(f\"\\n M√âTRICAS FINALES DE RENDIMIENTO:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\" Mejora vs modelo tradicional: {performance_metrics['performance_improvement']:+.1f}%\")\n",
        "print(f\" Usuarios atendidos: {performance_metrics['users_served']}\")\n",
        "print(f\" Total de interacciones: {performance_metrics['total_interactions']}\")\n",
        "print(f\" Efectividad de aprendizaje: {performance_metrics['learning_effectiveness']:.4f}\")\n",
        "print(f\" Capacidades emergentes: {performance_metrics['emergent_capabilities']}\")\n",
        "print(f\"  Baselines superados: {performance_metrics['baseline_methods_surpassed']}/4\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
